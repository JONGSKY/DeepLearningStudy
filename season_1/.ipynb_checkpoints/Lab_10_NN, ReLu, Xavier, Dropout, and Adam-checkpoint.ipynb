{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 5.194235369\n",
      "Epoch: 0002, Cost: 1.826119972\n",
      "Epoch: 0003, Cost: 1.188778022\n",
      "Epoch: 0004, Cost: 0.926134703\n",
      "Epoch: 0005, Cost: 0.784019831\n",
      "Epoch: 0006, Cost: 0.694398375\n",
      "Epoch: 0007, Cost: 0.630967351\n",
      "Epoch: 0008, Cost: 0.584415136\n",
      "Epoch: 0009, Cost: 0.547497446\n",
      "Epoch: 0010, Cost: 0.517301221\n",
      "Epoch: 0011, Cost: 0.492527283\n",
      "Epoch: 0012, Cost: 0.470913881\n",
      "Epoch: 0013, Cost: 0.453107653\n",
      "Epoch: 0014, Cost: 0.436615733\n",
      "Epoch: 0015, Cost: 0.423506100\n",
      "Epoch: 0016, Cost: 0.410552461\n",
      "Epoch: 0017, Cost: 0.399423609\n",
      "Epoch: 0018, Cost: 0.389733193\n",
      "Epoch: 0019, Cost: 0.381591664\n",
      "Epoch: 0020, Cost: 0.372371393\n",
      "Epoch: 0021, Cost: 0.365007896\n",
      "Epoch: 0022, Cost: 0.358061379\n",
      "Epoch: 0023, Cost: 0.351797129\n",
      "Epoch: 0024, Cost: 0.345616910\n",
      "Epoch: 0025, Cost: 0.340371606\n",
      "Epoch: 0026, Cost: 0.335441377\n",
      "Epoch: 0027, Cost: 0.330644303\n",
      "Epoch: 0028, Cost: 0.326027713\n",
      "Epoch: 0029, Cost: 0.321936965\n",
      "Epoch: 0030, Cost: 0.318036271\n",
      "Epoch: 0031, Cost: 0.314404638\n",
      "Epoch: 0032, Cost: 0.310900615\n",
      "Epoch: 0033, Cost: 0.307707803\n",
      "Epoch: 0034, Cost: 0.304635006\n",
      "Epoch: 0035, Cost: 0.301183575\n",
      "Epoch: 0036, Cost: 0.298790489\n",
      "Epoch: 0037, Cost: 0.296120686\n",
      "Epoch: 0038, Cost: 0.293708389\n",
      "Epoch: 0039, Cost: 0.290756482\n",
      "Epoch: 0040, Cost: 0.289172493\n",
      "Epoch: 0041, Cost: 0.286548157\n",
      "Epoch: 0042, Cost: 0.284507390\n",
      "Epoch: 0043, Cost: 0.282908039\n",
      "Epoch: 0044, Cost: 0.280851933\n",
      "Epoch: 0045, Cost: 0.278903817\n",
      "Epoch: 0046, Cost: 0.277124260\n",
      "Epoch: 0047, Cost: 0.275443782\n",
      "Epoch: 0048, Cost: 0.273628686\n",
      "Epoch: 0049, Cost: 0.272303370\n",
      "Epoch: 0050, Cost: 0.270874799\n",
      "Learning Finished!\n",
      "Accuracy: 0.9171\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 83.317024266\n",
      "Epoch: 0002, Cost: 22.620087625\n",
      "Epoch: 0003, Cost: 14.695985362\n",
      "Epoch: 0004, Cost: 10.737642152\n",
      "Epoch: 0005, Cost: 8.225937075\n",
      "Epoch: 0006, Cost: 6.511262390\n",
      "Epoch: 0007, Cost: 5.205811519\n",
      "Epoch: 0008, Cost: 4.214461417\n",
      "Epoch: 0009, Cost: 3.443314068\n",
      "Epoch: 0010, Cost: 2.831847212\n",
      "Epoch: 0011, Cost: 2.340086791\n",
      "Epoch: 0012, Cost: 1.948489089\n",
      "Epoch: 0013, Cost: 1.581562828\n",
      "Epoch: 0014, Cost: 1.287277775\n",
      "Epoch: 0015, Cost: 1.085707627\n",
      "Epoch: 0016, Cost: 0.886536639\n",
      "Epoch: 0017, Cost: 0.706540324\n",
      "Epoch: 0018, Cost: 0.598937545\n",
      "Epoch: 0019, Cost: 0.508455325\n",
      "Epoch: 0020, Cost: 0.411117009\n",
      "Epoch: 0021, Cost: 0.320652943\n",
      "Epoch: 0022, Cost: 0.257669525\n",
      "Epoch: 0023, Cost: 0.241525746\n",
      "Epoch: 0024, Cost: 0.186328626\n",
      "Epoch: 0025, Cost: 0.174395803\n",
      "Epoch: 0026, Cost: 0.157583387\n",
      "Epoch: 0027, Cost: 0.129242570\n",
      "Epoch: 0028, Cost: 0.140401144\n",
      "Epoch: 0029, Cost: 0.098094667\n",
      "Epoch: 0030, Cost: 0.102551946\n",
      "Epoch: 0031, Cost: 0.093103451\n",
      "Epoch: 0032, Cost: 0.091298100\n",
      "Epoch: 0033, Cost: 0.080233890\n",
      "Epoch: 0034, Cost: 0.075419814\n",
      "Epoch: 0035, Cost: 0.079285857\n",
      "Epoch: 0036, Cost: 0.069580282\n",
      "Epoch: 0037, Cost: 0.079900456\n",
      "Epoch: 0038, Cost: 0.067824988\n",
      "Epoch: 0039, Cost: 0.060956467\n",
      "Epoch: 0040, Cost: 0.061609381\n",
      "Epoch: 0041, Cost: 0.061810763\n",
      "Epoch: 0042, Cost: 0.059255671\n",
      "Epoch: 0043, Cost: 0.047237591\n",
      "Epoch: 0044, Cost: 0.049910291\n",
      "Epoch: 0045, Cost: 0.055516730\n",
      "Epoch: 0046, Cost: 0.060773764\n",
      "Epoch: 0047, Cost: 0.039432516\n",
      "Epoch: 0048, Cost: 0.047886730\n",
      "Epoch: 0049, Cost: 0.044160943\n",
      "Epoch: 0050, Cost: 0.065100051\n",
      "Learning Finished!\n",
      "Accuracy: 0.9483\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 128]))\n",
    "b1 = tf.Variable(tf.random_normal([128]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([128, 128]))\n",
    "b2 = tf.Variable(tf.random_normal([128]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([128, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 0.381092191\n",
      "Epoch: 0002, Cost: 0.144703236\n",
      "Epoch: 0003, Cost: 0.099704971\n",
      "Epoch: 0004, Cost: 0.075837605\n",
      "Epoch: 0005, Cost: 0.057126825\n",
      "Epoch: 0006, Cost: 0.046675678\n",
      "Epoch: 0007, Cost: 0.038485742\n",
      "Epoch: 0008, Cost: 0.030892441\n",
      "Epoch: 0009, Cost: 0.024233823\n",
      "Epoch: 0010, Cost: 0.020926308\n",
      "Epoch: 0011, Cost: 0.015807804\n",
      "Epoch: 0012, Cost: 0.015790672\n",
      "Epoch: 0013, Cost: 0.014397356\n",
      "Epoch: 0014, Cost: 0.012013092\n",
      "Epoch: 0015, Cost: 0.009005008\n",
      "Epoch: 0016, Cost: 0.011355880\n",
      "Epoch: 0017, Cost: 0.009898931\n",
      "Epoch: 0018, Cost: 0.008717619\n",
      "Epoch: 0019, Cost: 0.005637263\n",
      "Epoch: 0020, Cost: 0.011768045\n",
      "Epoch: 0021, Cost: 0.005632314\n",
      "Epoch: 0022, Cost: 0.006525963\n",
      "Epoch: 0023, Cost: 0.009605859\n",
      "Epoch: 0024, Cost: 0.006814048\n",
      "Epoch: 0025, Cost: 0.003777168\n",
      "Epoch: 0026, Cost: 0.006308257\n",
      "Epoch: 0027, Cost: 0.005714533\n",
      "Epoch: 0028, Cost: 0.007348973\n",
      "Epoch: 0029, Cost: 0.006292907\n",
      "Epoch: 0030, Cost: 0.005849656\n",
      "Epoch: 0031, Cost: 0.005878315\n",
      "Epoch: 0032, Cost: 0.005049140\n",
      "Epoch: 0033, Cost: 0.004750284\n",
      "Epoch: 0034, Cost: 0.000958926\n",
      "Epoch: 0035, Cost: 0.003028410\n",
      "Epoch: 0036, Cost: 0.011281153\n",
      "Epoch: 0037, Cost: 0.005755257\n",
      "Epoch: 0038, Cost: 0.001885747\n",
      "Epoch: 0039, Cost: 0.003589290\n",
      "Epoch: 0040, Cost: 0.006135330\n",
      "Epoch: 0041, Cost: 0.004262126\n",
      "Epoch: 0042, Cost: 0.002987863\n",
      "Epoch: 0043, Cost: 0.007074150\n",
      "Epoch: 0044, Cost: 0.003439240\n",
      "Epoch: 0045, Cost: 0.005646419\n",
      "Epoch: 0046, Cost: 0.001497370\n",
      "Epoch: 0047, Cost: 0.005010934\n",
      "Epoch: 0048, Cost: 0.005158590\n",
      "Epoch: 0049, Cost: 0.002412639\n",
      "Epoch: 0050, Cost: 0.004237974\n",
      "Learning Finished!\n",
      "Accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 128],\n",
    "                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([128]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[128, 128],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([128]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[128, 10],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for MNIST\n",
    "- 범위를 넓혀줘도 정확도가 더 좋아지지 않는 이유?\n",
    "- 데이터특성, overfitting, 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 0.288980470\n",
      "Epoch: 0002, Cost: 0.106381718\n",
      "Epoch: 0003, Cost: 0.071420384\n",
      "Epoch: 0004, Cost: 0.053843220\n",
      "Epoch: 0005, Cost: 0.042801692\n",
      "Epoch: 0006, Cost: 0.035024151\n",
      "Epoch: 0007, Cost: 0.030855861\n",
      "Epoch: 0008, Cost: 0.025677261\n",
      "Epoch: 0009, Cost: 0.026503333\n",
      "Epoch: 0010, Cost: 0.019218691\n",
      "Epoch: 0011, Cost: 0.022004949\n",
      "Epoch: 0012, Cost: 0.017601015\n",
      "Epoch: 0013, Cost: 0.015943830\n",
      "Epoch: 0014, Cost: 0.017249071\n",
      "Epoch: 0015, Cost: 0.011611305\n",
      "Epoch: 0016, Cost: 0.016647021\n",
      "Epoch: 0017, Cost: 0.013554193\n",
      "Epoch: 0018, Cost: 0.012208992\n",
      "Epoch: 0019, Cost: 0.009215402\n",
      "Epoch: 0020, Cost: 0.011314366\n",
      "Epoch: 0021, Cost: 0.012428481\n",
      "Epoch: 0022, Cost: 0.011862437\n",
      "Epoch: 0023, Cost: 0.009717711\n",
      "Epoch: 0024, Cost: 0.009189825\n",
      "Epoch: 0025, Cost: 0.011072441\n",
      "Epoch: 0026, Cost: 0.007456065\n",
      "Epoch: 0027, Cost: 0.009519044\n",
      "Epoch: 0028, Cost: 0.010137261\n",
      "Epoch: 0029, Cost: 0.010420911\n",
      "Epoch: 0030, Cost: 0.011183737\n",
      "Epoch: 0031, Cost: 0.005733400\n",
      "Epoch: 0032, Cost: 0.007836162\n",
      "Epoch: 0033, Cost: 0.008980204\n",
      "Epoch: 0034, Cost: 0.009214041\n",
      "Epoch: 0035, Cost: 0.006804109\n",
      "Epoch: 0036, Cost: 0.010831213\n",
      "Epoch: 0037, Cost: 0.007113789\n",
      "Epoch: 0038, Cost: 0.007939074\n",
      "Epoch: 0039, Cost: 0.006106701\n",
      "Epoch: 0040, Cost: 0.006338914\n",
      "Epoch: 0041, Cost: 0.010344066\n",
      "Epoch: 0042, Cost: 0.004606973\n",
      "Epoch: 0043, Cost: 0.006352321\n",
      "Epoch: 0044, Cost: 0.007438510\n",
      "Epoch: 0045, Cost: 0.003931139\n",
      "Epoch: 0046, Cost: 0.011366767\n",
      "Epoch: 0047, Cost: 0.006406833\n",
      "Epoch: 0048, Cost: 0.006186277\n",
      "Epoch: 0049, Cost: 0.007050310\n",
      "Epoch: 0050, Cost: 0.007766830\n",
      "Learning Finished!\n",
      "Accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W_1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W_2\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W_3\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W_4\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W_5\", shape=[512, 10],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout for MNIST\n",
    "- 보통 train을 할때는 keep_prob을 0.5~0.7사이로 해줌\n",
    "- 실전 또는 test에서는 반드시 1로 해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-26-06128d4ed1f5>:15: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch: 0001, Cost: 0.446126125\n",
      "Epoch: 0002, Cost: 0.170864381\n",
      "Epoch: 0003, Cost: 0.127249369\n",
      "Epoch: 0004, Cost: 0.104684015\n",
      "Epoch: 0005, Cost: 0.094876968\n",
      "Epoch: 0006, Cost: 0.083472297\n",
      "Epoch: 0007, Cost: 0.075407961\n",
      "Epoch: 0008, Cost: 0.067209636\n",
      "Epoch: 0009, Cost: 0.061279700\n",
      "Epoch: 0010, Cost: 0.059253289\n",
      "Epoch: 0011, Cost: 0.053644781\n",
      "Epoch: 0012, Cost: 0.049972992\n",
      "Epoch: 0013, Cost: 0.051366190\n",
      "Epoch: 0014, Cost: 0.047962813\n",
      "Epoch: 0015, Cost: 0.045608787\n",
      "Epoch: 0016, Cost: 0.042844229\n",
      "Epoch: 0017, Cost: 0.045070721\n",
      "Epoch: 0018, Cost: 0.038220470\n",
      "Epoch: 0019, Cost: 0.039210481\n",
      "Epoch: 0020, Cost: 0.039234222\n",
      "Epoch: 0021, Cost: 0.036027565\n",
      "Epoch: 0022, Cost: 0.037707421\n",
      "Epoch: 0023, Cost: 0.036576169\n",
      "Epoch: 0024, Cost: 0.033213722\n",
      "Epoch: 0025, Cost: 0.036645204\n",
      "Epoch: 0026, Cost: 0.031804289\n",
      "Epoch: 0027, Cost: 0.032520033\n",
      "Epoch: 0028, Cost: 0.033205972\n",
      "Epoch: 0029, Cost: 0.034248663\n",
      "Epoch: 0030, Cost: 0.030643816\n",
      "Epoch: 0031, Cost: 0.031891547\n",
      "Epoch: 0032, Cost: 0.027816665\n",
      "Epoch: 0033, Cost: 0.028617217\n",
      "Epoch: 0034, Cost: 0.030363656\n",
      "Epoch: 0035, Cost: 0.028061929\n",
      "Epoch: 0036, Cost: 0.028934064\n",
      "Epoch: 0037, Cost: 0.026897477\n",
      "Epoch: 0038, Cost: 0.025416869\n",
      "Epoch: 0039, Cost: 0.027540799\n",
      "Epoch: 0040, Cost: 0.027446188\n",
      "Epoch: 0041, Cost: 0.028525826\n",
      "Epoch: 0042, Cost: 0.024817083\n",
      "Epoch: 0043, Cost: 0.027647870\n",
      "Epoch: 0044, Cost: 0.026666278\n",
      "Epoch: 0045, Cost: 0.028038978\n",
      "Epoch: 0046, Cost: 0.025624873\n",
      "Epoch: 0047, Cost: 0.025740218\n",
      "Epoch: 0048, Cost: 0.024069147\n",
      "Epoch: 0049, Cost: 0.028634931\n",
      "Epoch: 0050, Cost: 0.024238226\n",
      "Learning Finished!\n",
      "Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_porb) rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32\n",
    "                          )\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W__1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W__2\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W__3\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W__4\", shape=[512, 512],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W__5\", shape=[512, 10],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob:1}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "- tf.train.AdadelitaOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.AdagradDAOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- tf.train.AdamOptimizer\n",
    "- tf.train.FtrlOptimizer\n",
    "- tf.train.ProximalGradientDescentOptimizer\n",
    "- tf.train.ProximalAdagradOptimizer\n",
    "- tf.train.RMSPropOptimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
